d$lat_receiver <- moorings$lat_receiver[match(d$receiver_number, moorings$receiver_number)]
d$long_receiver <- moorings$long_receiver[match(d$receiver_number, moorings$receiver_number)]
d$operation_duration <- moorings$operation_duration[match(d$receiver_number, moorings$receiver_number)]
d$receiver_depth <- moorings$receiver_depth[match(d$receiver_number, moorings$receiver_number)]
#################################################################
#### final dates processing based on receiver deployment/removal times:
# There shoud be no records of individuals before
# ... or after the day of deployment/removal (already dealt with above)
# but even on the day of appointment or removal
# there may be effects of deployment and removal that cause errors/disturb fish behaviour
# check the amount of date on the first day of deployment (until midnight):
table(d$date_time < d$date_operation_start_receiver)              # before day of deployment
table(d$date_time < as.POSIXct(d$date_operation_start_receiver) + 60*60*24)   # before midnight on day of deployment
# check the amount of data on after 6am on the final date of recording:
table(d$date_time > d$date_operation_end_receiver)                 # after day of removal
table(d$date_time > as.POSIXct(d$date_operation_end_receiver) - 60*60*18)      # after 6am on day of removal
# only include data that was collected from a receiver :
nrow(d)
d <- d %>%
# ... after midnight on the first day of deployment
filter(date_time > as.POSIXct(d$date_operation_start_receiver, tz = "UTC") + 60*60*24) %>%
# ... before 6am on the day of removal
filter(date_time < as.POSIXct(d$date_operation_end_receiver, tz = "UTC") - 60*60*18)
nrow(d)
#################################################################
#################################################################
#### Finalise dataframe
d <- d %>% select(
date_time,
date_time_numeric,
station_name,
receiver_number,
receiver,
lat_receiver,
long_receiver,
reduced_acoustic_id,
transmitter,
sensor_value,
sensor_unit,
sex,
total_length,
disc_width,
dst_id,
date_deployment_tag,
date_removal_tag_acc,
date_operation_start_receiver,
date_operation_end_receiver,
operation_duration,
receiver_depth
)
#################################################################
#################################################################
#### note that not all individuals tagged were ever detected
# 43 skate were tagged [check - differs from report!]:
length(levels(skateids$reduced_acoustic_id))
# these are the skate that were tagged:
levels(skateids$reduced_acoustic_id)
# 36 skate were detected:
length(levels(d$reduced_acoustic_id))
# these are the skate that were detected:
levels(d$reduced_acoustic_id)
# list of skate never detected:
levels(factor(
skateids$reduced_acoustic_id[which(
skateids$reduced_acoustic_id %ni% d$reduced_acoustic_id)]))
#################################################################
#################################################################
#### transmitters that also recorded depth n
# transmitters in the 200s also recorded depth, potentially enabling 3d geolocation
# create a smaller dataframe with transmitters that also recorded depth
td <-
d %>%
filter(as.numeric(reduced_acoustic_id) >= 200 &
as.numeric(reduced_acoustic_id) < 300)
td$reduced_acoustic_id <- factor(td$reduced_acoustic_id)
td$receiver <- factor(td$receiver)
# examine the duration for which these transmitters were at liberty
td$days <- td$date_removal_tag_acc - td$date_deployment_tag
td$days <- as.factor(td$days)
levels(td$days)
# below, save as a csv and import into QGIS to get a sense of distance between receivers
#################################################################
#################################################################
#### Save the resultant dataframe as a csv
setwd(paste0("/Users/el72/Documents/PhD/",
"Academic_PhD_Work/Data and Modelling/",
"Data/Processed Data/Basic Data Processed Outputs"))
saveRDS(d, "acoustics.rds")
# only include data that was collected from a receiver :
nrow(d)
d <- d %>%
# ... after midnight on the first day of deployment
filter(date_time > as.POSIXct(date_operation_start_receiver, tz = "UTC") + 60*60*24) %>%
# ... before 6am on the day of removal
filter(date_time < as.POSIXct(date_operation_end_receiver, tz = "UTC") - 60*60*18)
nrow(d)
d <- d %>% select(
date_time,
date_time_numeric,
station_name,
receiver_number,
receiver,
lat_receiver,
long_receiver,
reduced_acoustic_id,
transmitter,
sensor_value,
sensor_unit,
sex,
total_length,
disc_width,
dst_id,
date_deployment_tag,
date_removal_tag_acc,
date_operation_start_receiver,
date_operation_end_receiver,
operation_duration,
receiver_depth
)
# 43 skate were tagged [check - differs from report!]:
length(levels(skateids$reduced_acoustic_id))
# these are the skate that were tagged:
levels(skateids$reduced_acoustic_id)
# 36 skate were detected:
length(levels(d$reduced_acoustic_id))
# these are the skate that were detected:
levels(d$reduced_acoustic_id)
# list of skate never detected:
levels(factor(
skateids$reduced_acoustic_id[which(
skateids$reduced_acoustic_id %ni% d$reduced_acoustic_id)]))
# create a smaller dataframe with transmitters that also recorded depth
td <-
d %>%
filter(as.numeric(reduced_acoustic_id) >= 200 &
as.numeric(reduced_acoustic_id) < 300)
td$reduced_acoustic_id <- factor(td$reduced_acoustic_id)
td$receiver <- factor(td$receiver)
# examine the duration for which these transmitters were at liberty
td$days <- td$date_removal_tag_acc - td$date_deployment_tag
td$days <- as.factor(td$days)
levels(td$days)
setwd(paste0("/Users/el72/Documents/PhD/",
"Academic_PhD_Work/Data and Modelling/",
"Data/Processed Data/Basic Data Processed Outputs"))
saveRDS(d, "acoustics.rds")
head(d)
?image
?axis
devtools::document()
?compute_det_sim
?`date-time`
if(!(colnames(acoustics) %in% c("id", "timestamp", "long_receiver", "lat_receiver"))){
stop(paste0("acoustic_ls", [[i]], " does not contain all required column names."))
}
?`date_time`
date-time
?`date-time`
library(flapper)
?compute_det_sim
library(flapper)
library(flapper)
?compute_det_sim
library(flapper)
?compute_det_sim
?plot_det_sim
devtools::build_manual()
devtools::build_manua
devtools::build_manual
callr::rcmd
library(flapper)
?compute_det_sim
as.POSIXct("2016-01-01") +0.001
identical(as.POSIXct("2016-01-01") +0.001, as.POSIXct("2016-01-01") +0.04)
runif(10, -60, 60)
table(runif(100000, -60, 60)) > 1
table(runif(100000, -60, 60))
(table(runif(100000, -60, 60)) > 1) %>% table()
library(magrittr)
(table(runif(100000, -60, 60)) > 1) %>% table()
(table(runif(1000000, -60, 60)) > 1) %>% table()
library(flapper)
library(flapper)
library(flapper)
library(flapper)
library(flapper)
library(parallel)
?clusterExport
compute_det_sim <-
function(acoustics_ls,
thresh_time,
thresh_dist,
cl = NULL,
varlist = NULL,
output = 1,
verbose = TRUE
){
#### Initial checks
# Check that acoustics dataframes contains required columns
mapply(acoustics_ls, 1:length(acoustics_ls), FUN = function(acoustics, i){
if(any(!(c("id", "timestamp", "long_receiver", "lat_receiver") %in% colnames(acoustics)))){
stop(paste0("acoustic_ls[[", i, "]] does not contain all required column names."))
}
})
#### Set up similarity matrix
id_names <- as.character(sapply(acoustics_ls, function(acoustics) return(acoustics$id)))
nid <- length(id_names)
mat_sim <- matrix(NA, nrow = nid, ncol = nid, dimnames = list(id_names, id_names))
#### Loop over all combinations of individuals, pair timeseries and identify
# ... nearby observations in time and space
# Set up cluster, if necessary
if(!is.null(varlist)) parallel::clusterExport(cl = cl, varlist = varlist)
# Define a list, with one element for each individual
# Each element will contain
# Loop over each individual...
lout <-
pbapply::pblapply(acoustics_ls, cl = cl, function(acc1){
#### For each individual, loop over each other individual...
lint <-
lapply(acoustics_ls, function(acc2){
if(acc1$id[1] != acc2$id[1]){
#### Print individual
if(verbose){
cat("\n===================================================================================\n")
cat(paste("Individual (", as.character(acc1$id[1]),
") and individual (", as.character(acc2$id[1]), ").\n"))
}
#### Match timeseries based on closest observations in time
if(verbose) cat("Matching detection timeseries...\n")
# Remove any observations from the second individual more than some limit outside of the timeseries of the first individual
# ... (for speed when matching)
acc2 <- acc2[acc2$timestamp >= (min(acc1$timestamp) - thresh_time*60) &
acc2$timestamp <= (max(acc1$timestamp) + thresh_time*60), ]
if(nrow(acc2) == 0) return(NULL)
# Check for duplicated timestamps in each individuals dataframe, and adjust these
# ... by a small fraction prior to matching, so that all are included. Unless there are 100,000s
# ... of duplicate timestamps, this approach does not produce any duplicated observations.
dup1 <- duplicated(acc1$timestamp)
dup2 <- duplicated(acc2$timestamp)
if(length(dup1) > 1){
pos_dups1 <- which(dup1)
lpd1 <- length(pos_dups1)
adj_dups1 <- stats::runif(lpd1, -30, 30)
acc1$timestamp[pos_dups1] <- acc1$timestamp[pos_dups1] + adj_dups1
}
if(length(dup2) > 1){
pos_dups2 <- which(dup2)
lpd2 <- length(pos_dups2)
adj_dups2 <- stats::runif(lpd2, -30, 30)
acc2$timestamp[pos_dups2] <- acc2$timestamp[pos_dups2] + adj_dups2
}
# Match timeseries, readjusting any adjusted timestamps back to their original values
acc1$pos_in_acc2 <- utils.add::match_closest(acc1$timestamp, acc2$timestamp)
acc1$timestamp_acc2 <- acc2$timestamp[acc1$pos_in_acc2]
if(length(dup1) > 1) acc1$timestamp[pos_dups1] <- acc1$timestamp[pos_dups1] - adj_dups1
if(length(dup2) > 1) acc2$timestamp[pos_dups2] <- acc2$timestamp[pos_dups2] - adj_dups2
#### Exclude any timestamps more than timestamp beyond each other (could be 0 mins)
# Implement this now, before a threshold based on distance, below, for speed.
if(verbose) cat("Processing timeseries by theshold time difference...\n")
acc1$difftime_abs <- abs(difftime(acc1$timestamp, acc1$timestamp_acc2, units = "mins"))
acc1 <- acc1[acc1$difftime_abs <= thresh_time, ]
if(nrow(acc1) == 0) return(NULL)
#### Distances between pairs of receivers
if(verbose) cat("Computing differences between pairs of receivers...\n")
# Add receivers
acc1$lat_receiver_acc2 <- acc2$lat_receiver[acc1$pos_in_acc2]
acc1$long_receiver_acc2 <- acc2$long_receiver[acc1$pos_in_acc2]
# Compute distances
acc1$dist_btw_rec <- round(geosphere::distGeo(acc1[, c("long_receiver", "lat_receiver")],
acc1[, c("long_receiver_acc2", "lat_receiver_acc2")]))
#### Exclude any receivers more than some threshold distance beyond each other (could be 0 m):
cat("Processing timeseries by threshold distance...\n")
acc1 <- acc1[acc1$dist_btw_rec <= thresh_dist, ]
if(nrow(acc1) == 0) return(NULL) else return(acc1)
}
})
return(lint)
})
if(!is.null(cl)) parallel::stopCluster(cl)
#### Populate similarity matrix
for(i in 1:nid){
for(j in 1:nid){
if(i != j){
d <- lout[[i]][[j]]
if(!is.null(d)) mat[i, j] <- nrow(d) else mat[i, j] <- 0
}
}
}
#### Matrix based on % similarity
mat_nobs <- mat[]
for(i in 1:nrow(mat_nobs)){
mat_nobs[i, ] <- nrow(acoustics_ls[[i]])
}
mat_pc <- (mat/mat_nobs)*100
#### Return outputs
if(!(output %in% 1:2)){
warning(paste("'output ", output, " not supported; defaulting to output = 1."))
output <- 1
}
if(output == 1) {
out <- mat_sim
} else if(output == 2) {
out <- list(mat_sim = mat_sim, mat_nobs = mat_nobs, mat_pc = mat_pc, dat = lout)
}
return(out)
}
rm(list = ls())
?plot_det_sim
library(flapper)
library(flapper)
###########################################
###########################################
#### fine_scale_detections_dynamics
#### This script
# 1) closely examines spatial detection patterns through time for a few individuals
# ... focussing on the inidividuals with long acoustic timeseries
# The key research questions are as follows:
# 1) Individuals are usually detected arounnd the Southern receiver array, but are they detected at the same receivers?
# 2) Are  individuals detected at the same receivers close by in time? If so, do they have similar depth use through time?
# ... This has implications for interactions among  individuals
# ... e.g. whether or not individuals closely associate or are likely to avoid each other.
#### Wipe workspace
rm(list = ls())
#### Load essential packages
library(magrittr)
library(plot.pretty)
library(utils.add)
#### Global param
cex <- 1.5
cex.axis <- cex - 0.2
proj <- sp::CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")
proj_utm <- sp::CRS("+proj=utm +zone=29 ellps=WGS84") # UTM
#### dir2save
dir2savefig <- "~/Documents/PhD/Academic_PhD_Work/Data and Modelling/Scripts/3, Chapters/3, Home Ranges I/Description of Acoustic Data/"
###############################################
###############################################
#### Load data
###############################################
#### Main datasets
#### Directory
setwd(paste0("/Users/el72/Documents/PhD/",
"Academic_PhD_Work/Data and Modelling/",
"Data/Processed Data/Basic Data Processed Outputs"))
#### Main datasets
acoustics <- readRDS("acoustics.rds")
skateids_acc <- readRDS("skateids_acoustics_successfully_tagged.rds")
#### Initial processing
# Acoustics
acoustics$date <- as.Date(acoustics$date_time)
acoustics$individual <- skateids_acc$individual[match(acoustics$reduced_acoustic_id, skateids_acc$reduced_acoustic_id)]
acoustics$site_tagged <- skateids_acc$site_tagged[match(acoustics$reduced_acoustic_id, skateids_acc$reduced_acoustic_id)]
acoustics$maturation_status <- skateids_acc$maturation_status[match(acoustics$reduced_acoustic_id, skateids_acc$reduced_acoustic_id)]
acoustics$key <- factor(paste0(acoustics$sex, ", ", acoustics$maturation_status))
skateids_acc$key <- factor(paste0(skateids_acc$sex, ", ", skateids_acc$maturation_status))
# Define detected individuals
skateids_acc_det <- skateids_acc[skateids_acc$reduced_acoustic_id %in% unique(acoustics$reduced_acoustic_id),]
###############################################
###############################################
#### Overall similarity matrix
###############################################
#### Define similarity matrix
#### Arrange acoustics by group, number of days with detections and then timestamp (for each individual)
# This is necessary for (a) interpretable plots (by population group) and correct matching of detection timeseries (by timestamp)
days_with_det <- (table(acoustics$reduced_acoustic_id, acoustics$date) > 0) %>% rowSums() %>% sort(decreasing = TRUE)
skateids_acc_det$days_with_det <- days_with_det[match(as.character(skateids_acc_det$reduced_acoustic_id), names(days_with_det))]
skateids_acc_det <- skateids_acc_det %>% dplyr::arrange(key, desc(days_with_det))
skateids_acc_det[, c("key", "reduced_acoustic_id", "days_with_det")]
skateids_acc_det$fct_key <- paste0(skateids_acc_det$key, skateids_acc_det$reduced_acoustic_id)
skateids_acc_det$fct_key <- factor(skateids_acc_det$fct_key, levels = skateids_acc_det$fct_key)
acoustics$fct_key <- paste0(acoustics$key, acoustics$reduced_acoustic_id)
acoustics$fct_key <- factor(acoustics$fct_key, levels = skateids_acc_det$fct_key)
acoustics <- acoustics %>% dplyr::arrange(fct_key, date_time)
#### Number of individuals
nid <- length(unique(acoustics$reduced_acoustic_id))
id_names <- as.character(unique(acoustics$reduced_acoustic_id))
id_names
#### Set up to compute similarity matrix
acoustics_sbt <- acoustics %>% dplyr::select(fct_key, reduced_acoustic_id, date_time, long_receiver, lat_receiver)
colnames(acoustics_sbt) <- c("fct_key", "id", "timestamp", "long_receiver", "lat_receiver")
head(acoustics_sbt)
acoustics_ls <- split(acoustics_sbt, f = acoustics_sbt$fct_key)
names(acoustics_ls); id_names
#### Subset of data for testing
acoustics_ls <- acoustics_ls[1:2]
#### Compute similarity matrix
det_sim_ls <-
flapper::compute_det_sim(acoustics_ls = acoustics_ls,
thresh_time = 2,
thresh_dist = 0,
cl = NULL,
varlist = NULL,
output = 2,
verbose = TRUE)
det_sim_ls$mat_sim
det_sim_ls$mat_nobs
det_sim_ls$mat_pc
det_sim_ls$dat$`F, immature540`
library(flapper)
library(flapper)
?parallel::stopCluster
library(flapper)
?compute_det_sim
nrow(NULL)
nrow(list())
nrow(list()) == 0
?any
acc1 <- 1
acc2 <- list()
any(is.null(acc1), is.null(acc2), nrow(acc1) == 0, nrow(acc2) == 0)
is.null(acc1)
is.null(acc2)
nrow(acc1) == 0
nrow(acc1)
is.null(nrow(acc1))
is.null(nrow(acc2))
nrow(acc1) == 0
nrow(acc2) == 0)
library(flapper)
library(flapper)
c(NULL, 2, 3)
which.min(c(NULL, 2, 3))
which(c(NULL, 2, 3))
library(flapper)
library(flapper)
duplicated(c(2, 3, 4))
any(duplicated(c(2, 3, 4)))
match_closest
?match_closest
library(flapper)
library(flapper)
?compute_det_sim
library(flapper)
library(flapper)
library(flapper)
library(flapper)
library(flapper)
library(flapper)
library(flapper)
is.list(list())
library(flapper)
devtools::build_manual()
flag <- Tools4ETS::flag_ts(acoustics$date_time, fct = acoustics$reduced_acoustic_id, duration_threshold = 9e999)
flag
flag <- Tools4ETS::flag_ts(acoustics$date_time, fct = acoustics$reduced_acoustic_id, duration_threshold = 9e999)$flag3
acoustics$flag <- Tools4ETS::flag_ts(acoustics$date_time, fct = acoustics$reduced_acoustic_id, duration_threshold = 9e999)$flag3
table(acoustics$flag)
acoustics <- split(acoustics, f = acoustics$reduced_acoustic_id)
#
#
#
}
#### Match timeseries based on closest observations in time using pair_ts()
if(verbose) cat("Matching detection timeseries...\n")
library(utils.add)
View(acoustics_ls)
View(match_closest)
library(flapper)
# Remove any observations from the second individual more than some limit outside of the timeseries of the first individual
# ... and vice versa.
# ... (for speed when matching)
acc2 <- acc2[acc2$timestamp >= (min(acc1$timestamp) - thresh_time*60*2) &
acc2$timestamp <= (max(acc1$timestamp) + thresh_time*60*2), ]
if(nrow(acc2) == 0) return(NULL)
acc1 <- acc1[acc1$timestamp >= (min(acc2$timestamp) - thresh_time*60*2) &
acc1$timestamp <= (max(acc2$timestamp) + thresh_time*60*2), ]
library(flapper)
library(flapper)
devtools::spell_check()
library(flapper)
library(flapper)
devtools::spell_check()
library(flapper)
devtools::spell_check()
library(flapper)
devtools::build_manual()
library(data.table)
?foverlap
?foverlaps
library(flapper)
devtools::spell_check()
library(flapper)
?add_receiver_id
library(flapper)
devtools::build_manual()
library(flapper)
library(flapper)
library(flapper)
library(flapper)
library(flapper)
library(flapper)
?sample
n <- 25
mat <- matrix(sample(0:100, size = n^2, replace = TRUE), ncol = 25, nrow = 25)
mat
library(mat)
library(flapper)
mat <- matrix(sample(0:100, size = n^2, replace = TRUE), ncol = 25, nrow = 25)
plot_det_sim(mat)
n <- 25
mat <- matrix(sample(0:100, size = n^2, replace = TRUE), ncol = 25, nrow = 25)
plot_det_sim(mat)
library(flapper)
n <- 25
mat <- matrix(sample(0:100, size = n^2, replace = TRUE), ncol = 25, nrow = 25)
plot_det_sim(mat)
library(flapper)
devtools::build_manual()
usethis::use_data_raw()
#### Load data
getwd()
root()
roo
